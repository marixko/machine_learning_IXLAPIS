{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Machine Learning Hands-on",
      "provenance": [],
      "collapsed_sections": [
        "WDMJHAujxi7U",
        "dZbQHFM6o5lk",
        "At1Wujug-ElJ",
        "HUztj4xWCmFi",
        "Ez2rDI09T67w",
        "YRtx_jW6T67x",
        "KeQE7HdIT68J",
        "45y5MQyBT68Q",
        "x-zUvQmgT68a",
        "E7u6VTzcJxQT",
        "wD5hIoaDKIhj",
        "Yu8aGNZrLSyW",
        "H8Rweo2_LirI",
        "QRL_7q8aL1sl",
        "mUagJbKEL7D6",
        "noxaXAXQMQND",
        "7u7CDn2GMUpt",
        "_Fa-eAeVNMIS",
        "rbVK9FzFNVns"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marixko/machine_learning_IXLAPIS/blob/master/Machine_Learning_Hands_on.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHxouXUiCNjY",
        "colab_type": "text"
      },
      "source": [
        "This material was created by:\n",
        "\n",
        "\n",
        "*   Erik Vinicius de Lima - erik.vini@usp.br (Ph.D. student, IAG-USP)\n",
        "*   Lilianne Nakazono - lilianne.nakazono@usp.br (Ph.D. candidate, IAG-USP)\n",
        "*   Maria Luisa Buzzo - maria.buzzo@usp.br (M.Sc. student, IAG-USP)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OurmpomdDqjH",
        "colab_type": "text"
      },
      "source": [
        "--- \n",
        "\n",
        "**HOW TO GET THIS NOTEBOOK:**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7neJlPvIuBg",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "First of all, go to the machine_learning_IXLAPIS repository on GitHub (https://github.com/marixko/machine_learning_IXLAPIS)\n",
        "\n",
        "- Running in jupyter notebook\n",
        "\n",
        "  - Download the .ipynb file;\n",
        "  - In your terminal, go to the directory where the .ipynb is;\n",
        "  - Type jupyter-notebook;\n",
        "  - Inside jupyter, open the .ipynb file\n",
        "\n",
        "- Running in Google Colab (1)\n",
        "  - Click in the .ipynb file listed on https://github.com/marixko/machine_learning_IXLAPIS\n",
        "  - Click in \"Open in Colab\" button\n",
        "  - Go to File > Save a copy in Drive\n",
        "\n",
        "- Running in Google Colab (2)\n",
        "\n",
        "  - Open Google Colab: https://colab.research.google.com/\n",
        "  - Go to File > Open Notebook\n",
        "  - Select the GITHUB tab\n",
        "  - Enter the link: https://github.com/marixko/machine_learning_IXLAPIS.git\n",
        "  - It will show you the .ipynb file, click the \"Open notebook in a new tab\" icon\n",
        "  - Go to File > Save a copy in Drive \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Z59YHXPKuNp",
        "colab_type": "text"
      },
      "source": [
        "**SOME TRICKS:**\n",
        "\n",
        "To run the code in jupyter or Google Colab:\n",
        "- to run a cell and stay in the same cell: ctrl+enter\n",
        "- to run a cell and go to the next one: shift+center - this is what I will be using.\n",
        "- to create new cells: \n",
        "  - jupyter: b+b\n",
        "  - Google Colab: ctrl+m b"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knl71lTK1MhM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Run this cell for the three Machine Learning hands-on sessions! \n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t3SFZ99cDsdM",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# Descriptive Analyses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3cqyqdFZpyc",
        "colab_type": "text"
      },
      "source": [
        "The ultimate goal of this hands-on session is to learn how to read, visualize and understand the data and the magics of seaborn and scikit-learn. Hold on to that thought and let's do this! "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i9MjWaEBVJYN",
        "colab_type": "text"
      },
      "source": [
        "## Reading data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NN-h-IIZ6uwJ",
        "colab_type": "text"
      },
      "source": [
        "We will use data containing information of house pricing of California district from a 1990 census. The information are:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDYG77wP6PfX",
        "colab_type": "text"
      },
      "source": [
        "1. longitude: A measure of how far west a house is; a higher value is farther west\n",
        "2. latitude: A measure of how far north a house is; a higher value is farther north\n",
        "3. housingMedianAge: Median age of a house within a block; a lower number is a newer building\n",
        "4. totalRooms: Total number of rooms within a block\n",
        "5. totalBedrooms: Total number of bedrooms within a block\n",
        "6. population: Total number of people residing within a block\n",
        "7. households: Total number of households, a group of people residing within a home unit, for a block\n",
        "8. medianIncome: Median income for households within a block of houses (measured in tens of thousands of US Dollars)\n",
        "9. medianHouseValue: Median house value for households within a block (measured in US Dollars)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bAkIJp-lUze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's read the dataset:\n",
        "\n",
        "california_housing = pd.read_table(\"https://download.mlcc.google.com/mledu-datasets/california_housing_train.csv\", sep=\",\")\n",
        "# ascii use delim_whitespace=True instead of sep\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3te2noE0LOS_",
        "colab_type": "text"
      },
      "source": [
        "## Statistics and Data Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xf96aOavUK39",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Checking statistics of all columns\n",
        "california_housing.describe()\n",
        "\n",
        "# If you want to check the name of the columns:\n",
        "# list(california_housing)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZy2NxdsFka6",
        "colab_type": "text"
      },
      "source": [
        "It is essential to understand the data.\n",
        " - Counts: missing bands;\n",
        " - mean: if there are wrong values (due to observations or human error), very sensitive to outliers;\n",
        " - std: if it is too high, it is a proxy for problems... \n",
        " - min: if there are negative values where it shouldn't have.\n",
        "\n",
        " Allows to check, in general, if there is anything wrong with the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dj5w93uVm0zl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "california_housing.describe().transpose()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eywKrZECUUnJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "california_housing.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PgdV3oBU0ts",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "california_housing.hist(figsize=(15,15), bins=30)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEFV8lYvVI7y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.pairplot(california_housing)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0NKYIs9rOf9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10,10))\n",
        "sns.heatmap(california_housing.corr(), cmap='bwr', vmax=1, vmin=-1, center=0, square=True, annot=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pV89fvQunhV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "california_housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", \n",
        "                        alpha=0.3, s=california_housing[\"population\"]/100, \n",
        "                        label=\"population\", c=\"median_house_value\", \n",
        "                        cmap=plt.get_cmap(\"jet\"), colorbar=True, \n",
        "                        figsize=(15,7)) \n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpFOqdyuEl0t",
        "colab_type": "text"
      },
      "source": [
        "The boxplot is one of the most important analysis we can make of our data, specially to identify outliers.\n",
        "\n",
        "It is defined by 5 values:\n",
        "\n",
        " - median;\n",
        " - 1st quartile: 25% of the data;\n",
        " - 3rd quartile: 75% of the data;\n",
        " - Superior Limit: $min\\{max(data): Q3 + 1.5(Q3-Q1)\\}$;\n",
        " - Inferior Limit: $max\\{min(data): Q1 - 1.5(Q3-Q1)\\}$ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVqxRBdi61me",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "california_housing[['median_house_value']].boxplot(figsize=(10,10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbNokz9iLE5p",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "A density estimator is an algorithm to model the probability distribution of a dataset. For one dimensional data, we use the most common simple density estimator: the histogram. A histogram divides the data into discrete bins, counts the number of points that fall in each bin, and then visualizes the results.\n",
        "\n",
        "\n",
        "- How to plot one dimensional histograms?\n",
        "\n",
        "For example, let's start visualizing a bimodal distribution, for example the longitude.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NPBUHojQB5me",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hist = plt.hist(california_housing['longitude'], bins=30, density=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzSq9sTYeMcV",
        "colab_type": "text"
      },
      "source": [
        "The factor 'normed=True' normalizes the data, so we can see in the y-axis is the probability and not the counts.\n",
        "Since we are dealing with probabilities, this distribution should sum to 1, as we can check:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJZ46jXvedPb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "density, bins, patches = hist\n",
        "widths = bins[1:] - bins[:-1]\n",
        "(density * widths).sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xBG3JcRkMJXL",
        "colab_type": "text"
      },
      "source": [
        "This was a good choice of number of bins, but what do we do when this is not true?\n",
        "\n",
        "Actually, this is one of the issues with using a regular histogram as a density estimator: the choice of bin size and location can lead to representations that have qualitatively different features. \n",
        "\n",
        "- Let us try different numbers of bins to see what happens...\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Boljp7t9GpAL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(12, 4),\n",
        "                       sharex=True, sharey=True)\n",
        "fig.subplots_adjust(wspace=0.05)\n",
        "ax[0].hist(california_housing['longitude'], bins=100, density=True)\n",
        "ax[1].hist(california_housing['longitude'], bins=5, density=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhz_VdCofQFS",
        "colab_type": "text"
      },
      "source": [
        "On the left, the histogram makes clear that this is a bimodal (or even with more modes - undersmoothing) distribution. On the right, we see a unimodal distribution (oversmoothing). Without seeing the preceding code, you would probably not guess that these two histograms were built from the same data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKj-6gLb60Mp",
        "colab_type": "text"
      },
      "source": [
        "To solve this, it is always better to plot as a function of the density. We can think of a histogram as a stack of blocks, where we stack one block within each bin on top of each point in the dataset.\n",
        "\n",
        "Let's use a standard normal curve at each point instead to represent each block:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LNPi3Tk7PzU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from scipy.stats import norm\n",
        "\n",
        "density = sum(norm(xi).pdf(california_housing['longitude']) for xi in california_housing['longitude'])\n",
        "\n",
        "plt.fill_between(california_housing['longitude'], density, alpha=0.5)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1g59-VKgE5x",
        "colab_type": "text"
      },
      "source": [
        "This last plot is an example of a kernel density estimation in one dimension: it uses a Gaussian kernel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSMe5kFP7lNL",
        "colab_type": "text"
      },
      "source": [
        "- The Kernel Density Estimation (KDE) is a non-parametric method to estimate the Probability Distribution Function of a given dataset.\n",
        "- KDE is a data smoothing problem, where inferences about the data population are made.\n",
        "- In python, the function kde is part of the packages `seaborn` and `scikit-learn`.\n",
        "\n",
        "The free parameters of kernel density estimation are the kernel, which specifies the shape of the distribution placed at each point, and the kernel bandwidth, which controls the size of the kernel at each point.\n",
        "\n",
        "Here we introduce the beauty of using Scikit-Learn.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tx7VPbV9-td",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.neighbors import KernelDensity\n",
        "# instantiate and fit the KDE model\n",
        "kde = KernelDensity(bandwidth=1.0, kernel='gaussian')\n",
        "kde.fit(california_housing['longitude'][:, None])\n",
        "\n",
        "# score_samples returns the log of the probability density\n",
        "logprob = kde.score_samples(california_housing['longitude'][:, None])\n",
        "\n",
        "plt.fill_between(california_housing['longitude'], np.exp(logprob), alpha=0.5)\n",
        "plt.ylim(-0.02, 0.22)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "477R0iBU-PNZ",
        "colab_type": "text"
      },
      "source": [
        "The choice of bandwidth within KDE is extremely important to finding a suitable density estimate.\n",
        "\n",
        "- The badwidth is a smoothing parameter and the better it is defined, the better the data visualization will be.\n",
        "- How do we determine the best bandwidth then?\n",
        "\n",
        "-> Cross-Validation, where the basic idea is to construct an estimate of $F(h)$ and then to select $h$ to minimize this estimate.\n",
        "\n",
        "We can use K-Fold or Leave-One-Out methods:\n",
        " - Leave-One-Out: Leave-One-Out (or LOO) is a simple cross-validation. Each learning set is created by taking all the samples except one, the test set being the sample left out. This cross-validation procedure does not waste much data as only one sample is removed from the training set.\n",
        " - K-Fold: K-Fold divides all the samples in $k$ groups of samples, called folds (if $k=n$, this is equivalent to the Leave One Out strategy), of equal sizes (if possible). The prediction function is learned using $k-1$ folds, and the fold left out is used for test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jnsy2pE6AQMK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import LeaveOneOut\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "bandwidths = 10 ** np.linspace(-1, 1, 10)\n",
        "grid = GridSearchCV(KernelDensity(kernel='gaussian'),\n",
        "                    {'bandwidth': bandwidths},\n",
        "                    cv=KFold())\n",
        "\n",
        "grid.fit(california_housing['longitude'][:, None]);\n",
        "\n",
        "grid.best_params_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yCrh-LN_XD9C",
        "colab_type": "text"
      },
      "source": [
        "What about 2-D distributions? What is the best way to show the distribution of the data?\n",
        "\n",
        "Let's compare a simple scatter plot with a 2-D histogram using hexbin (the hexagonal binning routine).\n",
        "\n",
        "- Using `seaborn` we can visualize the marginal distribution of the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FgYUlF5AjZU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.jointplot(x=california_housing['longitude'], y=california_housing['latitude'],alpha=0.3);\n",
        "with sns.axes_style(\"white\"):\n",
        "  sns.jointplot(x=california_housing['longitude'], y=california_housing['latitude'], kind=\"hex\", color=\"k\");\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "62IqShoiXmJE",
        "colab_type": "text"
      },
      "source": [
        "From the previous plots, we can already see the great advantage of using a \"hex\" distribution in the data visualization\n",
        "Instead of overlapping, the plotting window is split in several hexbins, and the number of points per hexbin is counted. But it can be even better... "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfUBrik3XyHs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.jointplot(x=california_housing['longitude'], y=california_housing['latitude'], kind=\"kde\");"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDMJHAujxi7U",
        "colab_type": "text"
      },
      "source": [
        "### <font color='red'>**EXERCISES WITH S-PLUS DATA**</font>\n",
        "Now it's time to play around with astronomical data. For this exercise, you will use a sample data from S-PLUS. \n",
        "\n",
        "1. Read splus_laplata.txt with pandas\n",
        "2. Use df.info() to check the name and type of all columns\n",
        "3. Use df.describe().T to check statistics of your data \n",
        "4. Use df.column.value_counts() to check how many stars, quasars and galaxies are in this data\n",
        "5. Store in another variable called \"gal_splus\" a dataframe containing only galaxies. Check df.describe().T again, now only for galaxies.\n",
        "6. What is the filter that have highest standard deviation in magnitudes?\n",
        "7. Use df.column.boxplot() on the answer from Item 6. \n",
        "\n",
        "Now playing with distributions...\n",
        "\n",
        "Using the galaxies dataset:\n",
        "\n",
        "1. Retrieve two different colour distributions (1D histogram) of the data, one for the objects with u-r > 2.22 and one for u-r < 2.22. What is the comparison between the two distributions? What types of objects are more frequent in the local universe? \n",
        "2. What is the best bandwidth (h value) that describes both of the distributions?\n",
        "3. Plot the colour-colour diagram u-r vs. g-i  using scatter, hexbin and kde. Is there any bimodality? If so, what is the (astronomical) explanation for it?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSS3Q3IhGzLq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read splus data with pandas"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tLJHEspG1ei",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# find the statistics: describe, info..."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNbiwyxqG_Y3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# value_counts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbCrfFU3HEoE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# select only galaxies"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5jH7CclHN92",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# boxplot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEkFsxtzHPWQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# using the sample of galaxies, exclude objects with missing bands"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJGoEn5XHUVB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculate u-r and g-i"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkPbaHSbHZAA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create variables to store the values greater and smaller than 2.22"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4tIhMxdHj1A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# using cross validation find the bandwidth that maximize the data visualization (do this for only one of the above distributions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RKYY5pWDH6tm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot u-r vs g-i using seaborn to visualize the bimodality"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZbQHFM6o5lk",
        "colab_type": "text"
      },
      "source": [
        "## Cluster analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LU3fHXz3YYEj",
        "colab_type": "text"
      },
      "source": [
        "There are several ways to visualize different distributions present in the data.\n",
        "- The main idea of  clustering is: diminish the intra-group variance and increase the variance between groups.\n",
        "\n",
        "The most common method is the K-means. But what is this? \n",
        "\n",
        "- K-Means is an unsupervised learning method. K-means works iteratively by defining random centroids to clusters, trying to find the centroid that minimizes the distance of the points to the center of the defined sphere.\n",
        "\n",
        "Let's first create a dataset with clutering regions. `Scikit-learn` again!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_78irQ6ZDxS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "# create blobs\n",
        "data = make_blobs(n_samples=200, n_features=2, centers=4, cluster_std=1.6, random_state=50)\n",
        "\n",
        "# create np array for data points\n",
        "points = data[0] # shape (n_samples,n_features)\n",
        "loc_cluster = data[1] # shape (n_samples) - to which cluster the point belongs.\n",
        "\n",
        "# create scatter plot\n",
        "plt.scatter(data[0][:,0], data[0][:,1], c=data[1], cmap='viridis')\n",
        "plt.xlim(-15,15)\n",
        "plt.ylim(-15,15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "is87rsMp6aT2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# create kmeans object\n",
        "kmeans = KMeans(n_clusters=4)# fit kmeans object to data\n",
        "kmeans.fit(points)# print location of clusters learned by kmeans object\n",
        "print('Cluster Centers:', kmeans.cluster_centers_)# save new clusters for chart\n",
        "y_km = kmeans.fit_predict(points)\n",
        "\n",
        "plt.scatter(points[y_km ==0,0], points[y_km == 0,1], s=30, marker='^')\n",
        "plt.scatter(points[y_km ==1,0], points[y_km == 1,1], s=30, marker='s')\n",
        "plt.scatter(points[y_km ==2,0], points[y_km == 2,1], s=30, marker='o')\n",
        "plt.scatter(points[y_km ==3,0], points[y_km == 3,1], s=30, marker='x')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xq3kYwIoH0fH",
        "colab_type": "text"
      },
      "source": [
        "In this case, K-Means has worked very well and retrieved all the features that we were expecting. But this is not always true.\n",
        "\n",
        "By using this iterative manner, K-Means is a highly sensitive to outliers and initial conditions method.\n",
        " - The usual approach, therefore, is to run K-Means a lot of times in the same dataset to actually be sure that our results are reliable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "At1Wujug-ElJ",
        "colab_type": "text"
      },
      "source": [
        "### <font color='red'>**EXERCISES WITH S-PLUS DATA**</font>\n",
        "For this exercise, you will use the same sample data from S-PLUS as before. \n",
        "\n",
        "1. Plot the same colour-colour diagram (u-r vs. g-i) as before, but now try to identify the different distributions using the K-Means method.\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNBXhihgEi4U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read the S-PLUS data here:"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iptMZvTjEpI8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# select only galaxies:"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Uj2-tJmEvTI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# exclude the objects with missing values:"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qqDd2O_xEz6T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# calculate u-r and g-i:"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZQP1zBqE6yr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# run KMeans in the data using 2 clusters:"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rhXk3GAs9qhs",
        "colab_type": "text"
      },
      "source": [
        "## Dimensionality Reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xk2gPHLuD2lI",
        "colab_type": "text"
      },
      "source": [
        "Dimensionality reduction is the idea of reducing the number of random variables by considering a set of principle variables. There are several methods to do this: Principal Component Analysis, Local Linear Embedding ....\n",
        "\n",
        "Let's start with the easiest one: PCA (linear projection of the data)\n",
        "\n",
        "PCA is a method based on linear algebra, and for this reason it is considered a 'linear' method. By using orthogonal transformations, it finds \"new\" axes on the directions of maximum variance, called the Principal Components. Thus it allows the selection of the Principal Components that best describe the data.\n",
        "\n",
        "This method can be powerful, but often misses important non-linear structure in the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axvpcG0M6fsO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rng = np.random.RandomState(1)\n",
        "X = np.dot(rng.rand(2, 2), rng.randn(2, 200)).T\n",
        "plt.scatter(X[:, 0], X[:, 1])\n",
        "plt.axis('equal');\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=2)\n",
        "pca.fit(X)\n",
        "\n",
        "print('PCA Components:', pca.components_)\n",
        "\n",
        "print('PCA Variance:', pca.explained_variance_)\n",
        "\n",
        "def draw_vector(v0, v1, ax=None):\n",
        "    ax = ax or plt.gca()\n",
        "    arrowprops=dict(arrowstyle='->',\n",
        "                    linewidth=4,\n",
        "                    shrinkA=0, shrinkB=0,color='limegreen')\n",
        "    ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
        "\n",
        "# plot data\n",
        "plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
        "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
        "    v = vector * 3 * np.sqrt(length)\n",
        "    draw_vector(pca.mean_, pca.mean_ + v)\n",
        "plt.axis('equal');\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nrSkK-5t6n0d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pca = PCA(n_components=1)\n",
        "pca.fit(X)\n",
        "X_pca = pca.transform(X)\n",
        "print(\"original shape:   \", X.shape)\n",
        "print(\"transformed shape:\", X_pca.shape)\n",
        "\n",
        "X_new = pca.inverse_transform(X_pca)\n",
        "plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
        "plt.scatter(X_new[:, 0], X_new[:, 1], marker='x', alpha=0.8)\n",
        "plt.axis('equal');\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YF92O9afL8gL",
        "colab_type": "text"
      },
      "source": [
        "Manifold learning is an approach to non-linear dimensionality reduction. Algorithms for this task are based on the idea that the dimensionality of many data sets is only artificially high.\n",
        "\n",
        "Manifold Learning can be thought of as an attempt to generalize linear frameworks like PCA to be sensitive to non-linear structure in data. Though supervised variants exist, the typical manifold learning problem is unsupervised: it learns the high-dimensional structure of the data from the data itself, without the use of predetermined classifications.\n",
        "\n",
        "LLE: Locally linear embedding (LLE) seeks a lower-dimensional projection of the data which preserves distances within local neighborhoods. It can be thought of as a series of local Principal Component Analyses which are globally compared to find the best non-linear embedding.\n",
        "\n",
        "The standard LLE algorithm comprises three stages:\n",
        "1. Nearest Neighbors Search.\n",
        "2. Weight Matrix Construction.\n",
        "3. Partial Eigenvalue Decomposition. \n",
        "\n",
        "Modified LLE: multiple weight vectors in each neighborhood.\n",
        "\n",
        "ISOMAP: Isomap seeks a lower-dimensional embedding which maintains geodesic distances between all points.\n",
        "The Isomap algorithm comprises three stages:\n",
        "1. Nearest neighbor search.\n",
        "2. Shortest-path graph search.\n",
        "3. Partial eigenvalue decomposition.\n",
        "\n",
        "and so on... let's see an example of the differences between many manifold methods."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uq67oUS16rrp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from collections import OrderedDict\n",
        "from functools import partial\n",
        "from time import time\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from matplotlib.ticker import NullFormatter\n",
        "\n",
        "from sklearn import manifold, datasets\n",
        "\n",
        "# Next line to silence pyflakes. This import is needed.\n",
        "Axes3D\n",
        "\n",
        "n_points = 1000\n",
        "X, color = datasets.make_s_curve(n_points, random_state=0)\n",
        "n_neighbors = 10\n",
        "n_components = 2\n",
        "\n",
        "# Create figure\n",
        "fig = plt.figure(figsize=(15, 8))\n",
        "fig.suptitle(\"Manifold Learning with %i points, %i neighbors\"\n",
        "             % (1000, n_neighbors), fontsize=14)\n",
        "\n",
        "# Add 3d scatter plot\n",
        "ax = fig.add_subplot(251, projection='3d')\n",
        "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=color, cmap=plt.cm.Spectral)\n",
        "ax.view_init(4, -72)\n",
        "\n",
        "# Set-up manifold methods\n",
        "LLE = partial(manifold.LocallyLinearEmbedding,\n",
        "              n_neighbors, n_components, eigen_solver='auto')\n",
        "\n",
        "methods = OrderedDict()\n",
        "methods['LLE'] = LLE(method='standard')\n",
        "methods['LTSA'] = LLE(method='ltsa')\n",
        "methods['Hessian LLE'] = LLE(method='hessian')\n",
        "methods['Modified LLE'] = LLE(method='modified')\n",
        "methods['Isomap'] = manifold.Isomap(n_neighbors, n_components)\n",
        "methods['MDS'] = manifold.MDS(n_components, max_iter=100, n_init=1)\n",
        "methods['SE'] = manifold.SpectralEmbedding(n_components=n_components,\n",
        "                                           n_neighbors=n_neighbors)\n",
        "methods['t-SNE'] = manifold.TSNE(n_components=n_components, init='pca',\n",
        "                                 random_state=0)\n",
        "\n",
        "# Plot results\n",
        "for i, (label, method) in enumerate(methods.items()):\n",
        "    t0 = time()\n",
        "    Y = method.fit_transform(X)\n",
        "    t1 = time()\n",
        "    print(\"%s: %.2g sec\" % (label, t1 - t0))\n",
        "    ax = fig.add_subplot(2, 5, 2 + i + (i > 3))\n",
        "    ax.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)\n",
        "    ax.set_title(\"%s (%.2g sec)\" % (label, t1 - t0))\n",
        "    ax.xaxis.set_major_formatter(NullFormatter())\n",
        "    ax.yaxis.set_major_formatter(NullFormatter())\n",
        "    ax.axis('tight')\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUztj4xWCmFi",
        "colab_type": "text"
      },
      "source": [
        "### <font color='red'>**EXERCISES WITH S-PLUS DATA**</font>\n",
        "For this exercise, you will use the same sample data from S-PLUS as before. \n",
        "\n",
        "1. Plot the colour-magnitude diagram of u-r vs. r and apply the PCA method. Do the same for other colours that you may find interesting.\n",
        "2. Do the same thing using the LLE method. Compare the results with the PCA.\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGYef2csFg26",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# read the table, exclude objects with missing values (you don't need to extract only galaxies now)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hujcPQeRFqe1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# you will need to standize your features onto unit scale (mean = 0 and variance = 1), since it is not created from normal distributions. \n",
        "#For this:\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "features = ['color','r']\n",
        "# Separating out the features\n",
        "x = df.loc[:, features].values\n",
        "# Standardizing the features\n",
        "x = StandardScaler().fit_transform(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3QLH2TdGD_n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#run pca with n_components = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9n50TWX9GZDR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#run the standard LLE with n_component = 1, n_neighbors = 10, eigen_solver='auto' "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umTGoYBiNWxL",
        "colab_type": "text"
      },
      "source": [
        "### <font color='blue'>**RECOMMENDED BIBLIOGRAPHY**</font>\n",
        "\n",
        "1. Manifold in Scikit-Learn: https://scikit-learn.org/stable/modules/manifold.html.\n",
        "2. PCA Analysis: https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c.\n",
        "3. PCA Theory: Lever et al. 2017 (Nature Methods).\n",
        "4. K-Means Clustering: https://towardsdatascience.com/understanding-k-means-clustering-in-machine-learning-6a6e67336aa1.\n",
        "5. Other methods of clustering: https://towardsdatascience.com/the-5-clustering-algorithms-data-scientists-need-to-know-a36d136ef68.\n",
        "6. Python Statistics and Data Visualization: https://realpython.com/python-statistics/.\n",
        "7. Cross-Validation: https://scikit-learn.org/stable/modules/cross_validation.html.\n",
        "7. Prof. Laerte (:  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YH2w25sfRin",
        "colab_type": "text"
      },
      "source": [
        "This is the end of my class! Thank you (:\n",
        "\n",
        "---------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PtNFPq_OT-tl"
      },
      "source": [
        "# Classification with Machine Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "c6JdDSdgT67r",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn import metrics\n",
        "from sklearn import datasets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gHseXsBJT67s"
      },
      "source": [
        "## Reading data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wZ3rzZLjT67t"
      },
      "source": [
        "The data set we are going to use in this part of the tutorial consists of **50 samples from each of three species of Iris** (*Iris setosa*, *Iris virginica* and *Iris versicolor*). **Four features** were measured from each sample: the length and the width of the sepals and petals, in centimeters. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6NfkqTErT67t"
      },
      "source": [
        "<img src=\"https://thegoodpython.com/assets/images/iris-species.png\" width=900/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_ob7_Nk0T67t"
      },
      "source": [
        "Let's store the useful information in a dataframe called **iris**. The real classification for each row (i.e., for each flower) will be stored into another variable called **target**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZOyNr-2bT67u",
        "colab": {}
      },
      "source": [
        "target = pd.DataFrame(datasets.load_iris().target, columns=['target'])\n",
        "iris = pd.DataFrame(datasets.load_iris().data, columns=datasets.load_iris().feature_names) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MnFeK6xg71S",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "52dd53d7-f43a-45a6-e1d8-9cb4a51b8864"
      },
      "source": [
        "#change columns\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal length (cm)</th>\n",
              "      <th>sepal width (cm)</th>\n",
              "      <th>petal length (cm)</th>\n",
              "      <th>petal width (cm)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>6.7</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.2</td>\n",
              "      <td>2.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>146</th>\n",
              "      <td>6.3</td>\n",
              "      <td>2.5</td>\n",
              "      <td>5.0</td>\n",
              "      <td>1.9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>6.5</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.2</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>148</th>\n",
              "      <td>6.2</td>\n",
              "      <td>3.4</td>\n",
              "      <td>5.4</td>\n",
              "      <td>2.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>149</th>\n",
              "      <td>5.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5.1</td>\n",
              "      <td>1.8</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>150 rows Ã— 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
              "0                  5.1               3.5                1.4               0.2\n",
              "1                  4.9               3.0                1.4               0.2\n",
              "2                  4.7               3.2                1.3               0.2\n",
              "3                  4.6               3.1                1.5               0.2\n",
              "4                  5.0               3.6                1.4               0.2\n",
              "..                 ...               ...                ...               ...\n",
              "145                6.7               3.0                5.2               2.3\n",
              "146                6.3               2.5                5.0               1.9\n",
              "147                6.5               3.0                5.2               2.0\n",
              "148                6.2               3.4                5.4               2.3\n",
              "149                5.9               3.0                5.1               1.8\n",
              "\n",
              "[150 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ez2rDI09T67w"
      },
      "source": [
        "## Pre-processing data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RZy4C1SbT67w"
      },
      "source": [
        "This is the most important part when working with Machine Learning. It is not as simple as a cake recipe, but these few steps will already give you a better understanding of your data. Decisions must be taken from there, case by case. \n",
        "\n",
        "\n",
        "<font color='red'>**>> TRY YOURSELF:**</font>\n",
        "\n",
        "---\n",
        "\n",
        "STEP 1: Check if your dataset is ok \n",
        "- [ ] Check names of the columns -> `list(df)` or `print(df.columns)`\n",
        "- [ ] Check type of each column  -> `df.dtypes`\n",
        " ...or simply do `df.info()`\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "STEP 2: Check statistics\n",
        "- [ ] Statistical summary -> `df.describe().T` \n",
        "- [ ] Check outliers -> `df.column.boxplot()` \n",
        "\n",
        "(+ other analyses based on data visualization )\n",
        "\n",
        "---\n",
        "\n",
        "STEP 3: Check missing values\n",
        "- [ ] Check presence of NaNs -> `df.isna().sum()`\n",
        "- [ ] Drop NaNs, if necessary (not always the best way to deal with it!) -> `df = df.dropna()`\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "STEP 4: Check correlations\n",
        "- [ ] Correlation matrix -> `df.corr()`\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YRtx_jW6T67x"
      },
      "source": [
        "## Cross-validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6fLszWQ3T67y"
      },
      "source": [
        "In classification problems, we validate models through cross-validation. \n",
        "Here we are going to sample our data into training and validation sets in a very simple way called **Holdout method**, instead of K-Fold. \n",
        "\n",
        "In Holdout method we just sample X% of the data as training set and another (100-X)% as validation set. There is no rule for choosing X, but we usually use X=70 or X=75. \n",
        "However, we highly recommend that you use K-Fold in your works, instead of Holdout. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euuWevVfAoEf",
        "colab_type": "text"
      },
      "source": [
        "<p align=\"center\"><img src=\"https://raw.githubusercontent.com/marixko/machine_learning_IXLAPIS/master/holdout.png\"/></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bOGgfz6AAsMM",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "**Note:** The iris dataset is very small, thus we will not consider a validation set and we will simply work with only a training and a testing set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rboqykjcT676",
        "colab": {}
      },
      "source": [
        "X_train,X_test,y_train,y_test=train_test_split(iris,target,test_size=0.3,random_state=42)\n",
        "print(len(X_test), len(X_train))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5pnaJrskT678",
        "colab": {}
      },
      "source": [
        "# Frequencies of each flower in our dataset\n",
        "target.target.value_counts(normalize=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2rlF6OzQT67-",
        "colab": {}
      },
      "source": [
        "# Frequencies of each flower in our test set\n",
        "y_test.target.value_counts(normalize=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AjYks7DAT67_",
        "colab": {}
      },
      "source": [
        "# Frequencies of each flower in our training set\n",
        "y_train.target.value_counts(normalize=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "x_8k7H98T68B"
      },
      "source": [
        "Did you notice that we are sampling a different distribution from our initial dataset? We have more CLASS 0 than others in our testing set. Also, in our training set, we have less of CLASS 0. \n",
        "\n",
        "In order to maintain the initial proportion, we do a **stratified sampling**:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "k6gqFrSET68B",
        "colab": {}
      },
      "source": [
        "# train_test_split with stratify\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "t9B9IHHWT68D"
      },
      "source": [
        "<font color='red'>**>> TRY YOURSELF:**</font>\n",
        "\n",
        "- [x] Split your data by classes frequences with stratified sampling \n",
        "- [ ] Check again the frequencies of classes 0, 1 and 2 from our training and testing sets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KeQE7HdIT68J"
      },
      "source": [
        "## Training and Validating models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Ppbmh4-WT68J"
      },
      "source": [
        "There are several classification methods you can use, each of them has its own pros and cons, depending on your science goals and on your dataset. \n",
        "\n",
        "Regardless of the algorithm you choose, keep in mind that you have to:\n",
        "\n",
        "- Check the algorithm assumptions of your data (e.g. linearity) \n",
        "- Check what each parameter of your model (also known as hyperparameter) does. This is important for you to be able to refine your model fitting\n",
        "- Check which features might be useful for your problem\n",
        "\n",
        "What you can validate through cross-validation:\n",
        "\n",
        "- Which algorithm to use\n",
        "- Which features to use\n",
        "- Which hyperparameters to use\n",
        "\n",
        "We will give you an example using Decision Trees and Random Forest."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yUrlVoPiT68J"
      },
      "source": [
        "### Decision Trees"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2ZmF-lKkT68K",
        "colab": {}
      },
      "source": [
        "clf_DT = DecisionTreeClassifier(random_state=42)\n",
        "clf_DT.fit(X_train, y_train.target.ravel())\n",
        "\n",
        "y_pred = clf_DT.predict(X_test)\n",
        "matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "fig = sns.heatmap(matrix, annot=True, square=True, cbar=False, cmap=\"Blues\", annot_kws={\"size\": 16})\n",
        "plt.xlabel('Predicted Labels', size=18)\n",
        "plt.ylabel('True Labels',size=18)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.show() \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9XM-V9_8T68L",
        "colab": {}
      },
      "source": [
        "print(metrics.classification_report\n",
        "      (y_test,y_pred, digits=4))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4fpupHvqT68N"
      },
      "source": [
        "### Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "m_zgZZEtT68N",
        "colab": {}
      },
      "source": [
        "clf_RF = RandomForestClassifier(random_state=42)\n",
        "clf_RF.fit(X_train, y_train.target.ravel())\n",
        "\n",
        "y_pred = clf_RF.predict(X_test)\n",
        "matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "fig = sns.heatmap(matrix, annot=True, square=True, cbar=False, cmap=\"Blues\", annot_kws={\"size\": 16})\n",
        "plt.xlabel('Predicted Labels', size=18)\n",
        "plt.ylabel('True Labels',size=18)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.show() \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mTTeI9WNT68P",
        "colab": {}
      },
      "source": [
        "print(metrics.classification_report\n",
        "      (y_test,y_pred, digits=4))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "45y5MQyBT68Q"
      },
      "source": [
        "## Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pAEzscyJT68R"
      },
      "source": [
        "Let's recall the definition of each metric shown in `classification_report()`\n",
        "\n",
        "First, for a classification problem we can define <font color='green'>True Positives</font>, <font color='red'>False Positives</font>, <font color='red'>False Negatives</font> and <font color='green'>True Negatives</font>. To understand how we define these values, **let's suppose we are interested in the CLASS 0 objects**. Then:\n",
        "\n",
        "- <font color='green'>True Positives</font>: is the number of CLASS 0 objects that were CORRECTLY classified as CLASS 0 \n",
        "- <font color='red'>False Positives</font>: is the number of CLASS 1 or CLASS 2 objects that were INCORRECTLY classified as CLASS 0 \n",
        "- <font color='red'>False Negatives</font>: is the number of CLASS 0 objects that were INCORRECTLY classified as CLASS 1 or CLASS 2\n",
        "- <font color='green'>True Negatives</font>: is the number of CLASS 1 or CLASS 2 objects that were CORRECTLY or INCORRECTLY classified as CLASS 1 or CLASS2 (for simplicity, we can gather all these cases altogether)\n",
        "\n",
        "\n",
        "We can see below how the confusion matrix looks like:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wg8riQTTT68R"
      },
      "source": [
        "<p align=\"center\"><img src = \"https://raw.githubusercontent.com/marixko/classification_regression/master/confusion_matrix.png\" width=500/></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "BlWcjxjgT68V"
      },
      "source": [
        "### Precision/Purity\n",
        "\n",
        "\n",
        "> Precision $\n",
        "\\equiv \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Positives}}$ is the fraction of correct classifications among objects classified as CLASS 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCUxWavVNAVp",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "### Recall/Completeness\n",
        "\n",
        "> Recall $\n",
        "\\equiv \\frac{\\text{True Positives}}{\\text{True Positives} + \\text{False Negatives}}$ is the fraction of CLASS 0 objects that we are classifying correctly\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qacHsTjcNCzU",
        "colab_type": "text"
      },
      "source": [
        "### F-score \n",
        "\n",
        "> $F = 2\\Big(P_i^{-1}+R_i^{-1}\\Big)^{-1}  = 2 \\times \\frac{P_iR_i}{P_i+R_i}$ is the harmonic mean of Precision and REcall\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQ1G_JHeNEjp",
        "colab_type": "text"
      },
      "source": [
        "### Accuracy\n",
        "\n",
        "> Accuracy $\\equiv \\frac{\\text{True Positives} + \\text{True Negatives}}{\\text{Total}}$ is the fraction of correct classifications, in overall"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZkC7pC1XMHn",
        "colab_type": "text"
      },
      "source": [
        "<font color='red'>**>> TO THINK ABOUT:**</font>\n",
        "\n",
        "Generalize the formulas above for any given class. (HINT: you can define the confusion matrix using double subindex, for instance FN$_{i,i}$ for false negatives and FP$_{i,j}$ for false positives, where $i,j = \\{0,1,2\\}$ ) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "x-zUvQmgT68a"
      },
      "source": [
        "#Regression with Machine Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3WzpBcYOT68a",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import SGDRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "86dQ7GWGT68d",
        "colab": {}
      },
      "source": [
        "# Let's read the dataset. We will use this dataset for training and validating models\n",
        "california_housing = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/california_housing_train.csv\", sep=\",\")\n",
        "\n",
        "# This dataset will be used for testing our model\n",
        "california_housing_test = pd.read_csv(\"https://download.mlcc.google.com/mledu-datasets/california_housing_test.csv\", sep=\",\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mFSr2PKST68f"
      },
      "source": [
        "## Pre-processing data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RI7Ed9nbT68f"
      },
      "source": [
        "This is the most important part when working with Machine Learning. It is not as simple as a cake recipe, but these few steps will already give you a better understanding of your data. Decisions must be taken from there, case by case. \n",
        "\n",
        "\n",
        "<font color='red'>**>> TRY YOURSELF:**</font>\n",
        "\n",
        "---\n",
        "\n",
        "STEP 1: Check if your dataset is ok \n",
        "- [x] Check names of the columns -> `list(df)` or `print(df.columns)`\n",
        "- [x] Check type of each column  -> `df.dtypes`\n",
        " ...or simply do `df.info()`\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "STEP 2: Check statistics\n",
        "- [x] Statistical summary -> `df.describe().T` \n",
        "- [x] Check outliers -> `df.column.boxplot()` \n",
        "\n",
        "(+ other analyses based on data visualization )\n",
        "\n",
        "---\n",
        "\n",
        "STEP 3: Check missing values\n",
        "- [x] Check presence of NaNs -> `df.isna().sum()`\n",
        "- [x] Drop NaNs, if necessary (not always the best way to deal with it!) -> `df = df.dropna()`\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "STEP 4: Check correlations\n",
        "- [x] Correlation matrix -> `df.corr()`\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UwIL7MeMT68g"
      },
      "source": [
        "<font color='red'>**>> TO THINK ABOUT:**</font>\n",
        "\n",
        "1) What variable (also called as target or dependent variable) do we want to predict? \n",
        "\n",
        "2) In your judgement -- based on the plots -- which features (also called as independent variables) would you include to predict the value you are interested in?\n",
        "\n",
        "3) What might be the most important independent variable(s) that explain your target? Why?\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YeqOEKe3T68g"
      },
      "source": [
        "## Training and Validating Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "xuNF7gd-T68h"
      },
      "source": [
        "First of all, let's save all features we are going to use to train our algorithm in a variable called **data** and the corresponding price values in a variable called **target**.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3newo-WoT68i",
        "colab": {}
      },
      "source": [
        "target = california_housing['median_house_value']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7-bfpcRZT68s",
        "colab": {}
      },
      "source": [
        "data = california_housing.drop('median_house_value', axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "78cQjpK7T68t"
      },
      "source": [
        "We can again sample our data into training and validation sets using **Holdout method**. Note that this time we have sufficient data to separate into training, validation and testing samples.\n",
        "\n",
        "<p align=\"center\"><img src=\"https://raw.githubusercontent.com/marixko/machine_learning_IXLAPIS/master/holdout_2.png\" width=500/></p>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TVm9rGpzT68t",
        "colab": {}
      },
      "source": [
        "X_train,X_vali,y_train,y_vali=train_test_split(data,target,test_size=0.3,random_state=42)\n",
        "print(len(X_vali), len(X_train))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "etU7AOEMT68v"
      },
      "source": [
        "But here we also show how to sample your data using K-Fold method with (n = 4) folds. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0jUyHvciT68v",
        "colab": {}
      },
      "source": [
        "# from sklearn.model_selection import KFold\n",
        "# kf = KFold(n_splits=4)\n",
        "# for train_index, test_index in kf.split(data):\n",
        "#   print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
        "#   X_train, X_test = data.loc[train_index], data.loc[test_index]\n",
        "#   y_train, y_test = target.loc[train_index], target.loc[test_index]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IrcLvKs4T68x"
      },
      "source": [
        "## Training Models and Validation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JlAJQHZrT68x"
      },
      "source": [
        "### Linear Regression with Ordinary Least Squares\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7rNQ4eH4T68y"
      },
      "source": [
        "The class `LinearRegression()` from sklearn implements the Ordinary Least Squares to estimate the parameters of a linear regression. This method minimizes the residual sum of squares:\n",
        "\n",
        "$RSS(\\beta) = \\sum_{i=1}^{N}(y_i-\\hat{y}_i)$, \n",
        "\n",
        "where y are the observables of the dependent variable (or \"true values\") and $\\hat{y}_i = {\\beta_0+\\beta_1x_1+\\beta_2x_2+...}$ refers to our linear model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o9WrCEtHT68y",
        "colab": {}
      },
      "source": [
        "model_LR = LinearRegression().fit(X_train, y_train) #fitting model\n",
        "y_LR = model_LR.predict(X_vali) #predicting\n",
        "\n",
        "plt.close()\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.scatter(y_vali, y_LR, alpha=0.5)\n",
        "plt.plot([0,700000], [0,700000], color=\"red\")\n",
        "plt.ylabel('Prediction')\n",
        "plt.xlabel('True')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TA0EYkxvT684"
      },
      "source": [
        "If we are interested in acquiring the best model that fits our data, the best way to do model validation is NOT  through cross-validation, but through $R^2$ and/or residual analysis! When using cross-validation, we are focused in getting the best prediction accuracy. Validating in different ways, may lead to different models (i.e., parameters may not be the same!)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HaA9QTrGT685",
        "colab": {}
      },
      "source": [
        "print('R-squared:',model_LR.score(X_train,y_train))\n",
        "print('Mean Squared Error:', mean_squared_error(y_vali,y_LR))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Yfzwmy0oT688"
      },
      "source": [
        "<font color='red'>**>> TRY YOURSELF:**</font>\n",
        "  \n",
        "- [ ] Use the log of the house prices. How does it change your results?\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "N-dlr6-0T689"
      },
      "source": [
        "### Linear Regression with Stochastic Gradient Descent \n",
        "\n",
        "Instead of minimizing the residual sum of squares, Stochastic Gradient Descent will minimize the regularized training error given by:\n",
        "\n",
        "$ E(w,b) = \\frac{1}{n} \\sum_{i=1}^{N} L(y_i, \\hat{y_i})+ \\alpha R(w) $,\n",
        "\n",
        "where L is a loss function, R is a regularization term that penalizes model complexity as a function of model parameters w, $\\alpha$ is a non-negative hyperparameter and $\\hat{y}_i = {\\beta_0+\\beta_1x_1+\\beta_2x_2+...}$ refers to our linear model.\n",
        "\n",
        "Choices for L: \n",
        "\n",
        "    * Hinge: (soft-margin) Support Vector Machines.\n",
        "    * Log: Logistic Regression.\n",
        "    * Least-Squares: Ridge Regression.\n",
        "    * Epsilon-Insensitive: (soft-margin) Support Vector Regression.\n",
        "\n",
        "Popular choices for R:\n",
        "\n",
        "    * L2 norm: \n",
        "$ R(w) = \\frac{1}{2} \\sum_{i=1}^n w_i^2 $ \n",
        "\n",
        "    * L1 norm:\n",
        "$ R(w) = \\sum_{i=1}^n |w_i| $     \n",
        "\n",
        "    * Elastic Net:\n",
        "$ R(w) = \\frac{\\rho}{2} \\sum_{i=1}^n w_i^2 + (1-\\rho) \\sum_{i=1}^n |w_i|  $ "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SBmU4HSRT68-"
      },
      "source": [
        "**IMPORTANT:** SGD is highly sensitive to feature scaling. It is recommended that you scale your data first. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "APN-zyoPT68-",
        "colab": {}
      },
      "source": [
        "#Scaling your data with standardized normalization (mean 0 and variance 1)\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)  \n",
        "X_train_scaled = scaler.transform(X_train)\n",
        "X_vali_scaled = scaler.transform(X_vali)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "z1GkTOpFT68_",
        "colab": {}
      },
      "source": [
        "model_SGD = SGDRegressor(loss=\"squared_loss\", penalty=\"l2\", max_iter=1000, random_state=42, eta0=0.00001, verbose=1)\n",
        "model_SGD.fit(X_train_scaled, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jzrIjVW_T69B",
        "colab": {}
      },
      "source": [
        "y_SGD = model_SGD.predict(X_vali_scaled)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "G6OdhBauT69C",
        "colab": {}
      },
      "source": [
        "plt.close()\n",
        "plt.figure(figsize=(10,10))\n",
        "plt.scatter(y_vali, y_SGD, alpha=0.5)\n",
        "plt.plot([0,600000], [0,600000], color=\"red\")\n",
        "plt.ylabel('Prediction')\n",
        "plt.xlabel('True')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZnhEoUEsT69J",
        "colab": {}
      },
      "source": [
        "print('R-squared:',model_SGD.score(X_train_scaled,y_train))\n",
        "print('Mean Squared Error:', mean_squared_error(y_vali,y_SGD))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDNcCsXmEy7m",
        "colab_type": "text"
      },
      "source": [
        "# Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-h0dXgW0Jp6g",
        "colab_type": "text"
      },
      "source": [
        "## Introduction to Keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mC12GQrOJua6",
        "colab_type": "text"
      },
      "source": [
        "Keras (https://keras.io/) is a Application Programming Interface (API) which aims to facilitate the interaction between the user and a low-level machine learning language, like TensorFlow (https://www.tensorflow.org/).\n",
        "\n",
        "It was created and is maintained by Francois Chollet who is also the author of the excellent book \"Deep Learning with Python\".\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E7u6VTzcJxQT",
        "colab_type": "text"
      },
      "source": [
        "### How does deep-learning work?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K4g1qrZuJ1Gf",
        "colab_type": "text"
      },
      "source": [
        "Deep-learning (and machine learning in general) is based on the assumption that there is a relation between the different variables of the problem you are trying to solve.\n",
        "\n",
        "<img src=\"Extra/TrainLoop.png\" style=\"width: 500px;\"/>\n",
        "\n",
        "***\n",
        "To train a model, we need three datasets:\n",
        "* A training dataset: So the model can learn the relation between the input and output;\n",
        "* A validation dataset: So we can have an independent estimate of the performance of the model during the training;\n",
        "* A testing dataset: To make a final evaluation of the performance of our model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoFRMvS1KDrU",
        "colab_type": "text"
      },
      "source": [
        "### Regression with Keras (Multi-Layer Perceptron)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wD5hIoaDKIhj",
        "colab_type": "text"
      },
      "source": [
        "#### Loading and defining the datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_ZghzkGKM9a",
        "colab_type": "text"
      },
      "source": [
        "For the regression part of the hands-on, we will use the (already seen) California housing dataset.\n",
        "\n",
        "This dataset contains 20000 rows in total, with 9 columns each. The features are:\n",
        "* Latitude;\n",
        "* Longitude;\n",
        "* Housing median age;\n",
        "* Total rooms;\n",
        "* Total bedrooms;\n",
        "* Population;\n",
        "* Households;\n",
        "* Median income;\n",
        "* Median house value.\n",
        "\n",
        "For this exercise, we will use the first 8 features to predict the __median house value__ on the testing dataset.\n",
        "\n",
        "Lets start by loading the data with the Pandas package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g7QjPbIEJo-Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing Pandas:\n",
        "import pandas as pd\n",
        "\n",
        "# Importing the training dataset:\n",
        "Train_Data = pd.read_csv('Data/california_housing_train.csv')\n",
        "\n",
        "# And the testing dataset:\n",
        "Test_Sample = pd.read_csv('Data/california_housing_test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egStEWbdKbXu",
        "colab_type": "text"
      },
      "source": [
        "As we just saw, we need to define a __validation dataset__ before training our model. This can be easily done with sklearn's 'train_test_split' function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1_egC2-KgRB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import the 'train_test_split' function\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# We use this function like this:\n",
        "Train_Sample, Validation_Sample = train_test_split(Train_Data, train_size=0.8, random_state=1882)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYHguTJnKh3C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lets check the size of each sample:\n",
        "print('Training sample: %s rows'%(len(Train_Sample)))\n",
        "print('Validation sample: %s rows' %(len(Validation_Sample)))\n",
        "print('Test sample: %s rows'       %(len(Test_Sample)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-eQJb2QKkNU",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "<font size=\"3\">\n",
        "When working with a ML problem, we should always check the distribution of the input variables. In this example we will check how the 8 features are distributed in the training, validation, and testing samples.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3xwWaHunKmG2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# The features of our data are:\n",
        "Features = ['longitude','latitude','housing_median_age','total_rooms','total_bedrooms','population','households','median_income']\n",
        "\n",
        "# The target of our regression is:\n",
        "Target   = ['median_house_value']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuNA83yRKnqU",
        "colab_type": "text"
      },
      "source": [
        "We can check the distribution of features of each dataset using histograms made with Matplotlib:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bnrl4V1vKqAy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample_histograms(Train, Validation, Test, Features, Target):\n",
        "  import matplotlib.pyplot as plt\n",
        "  import numpy as np\n",
        "  \n",
        "  fig, ax = plt.subplots(figsize=(12,10))\n",
        "  plt.subplots_adjust(hspace=0.5, wspace=0.5)\n",
        "  plt.style.use('default')\n",
        "  plt_idx = 1\n",
        "  for feature in (Features + Target):\n",
        "      plt.subplot(3, 3, plt_idx)\n",
        "\n",
        "      Feature_min = min(np.min(Train[feature]), np.min(Validation[feature]), np.min(Test[feature]))\n",
        "      Feature_max = max(np.max(Train[feature]), np.max(Validation[feature]), np.min(Test[feature]))\n",
        "\n",
        "      plt.hist(Train[feature], lw=2, range=(Feature_min, Feature_max), bins=20, histtype='step', label='Training sample')\n",
        "      plt.hist(Validation[feature], lw=2, range=(Feature_min, Feature_max), bins=20, histtype='step', label='Validation sample')\n",
        "      plt.hist(Test[feature], lw=2, range=(Feature_min, Feature_max), bins=20, histtype='step', label='Testing sample')\n",
        "\n",
        "      plt.yscale('log')\n",
        "\n",
        "      plt.xlabel(feature, size=12)\n",
        "\n",
        "      plt.grid(lw=.5)\n",
        "      plt_idx = plt_idx+1\n",
        "\n",
        "      if plt_idx == 4:\n",
        "        plt.legend(loc='center', bbox_to_anchor=(1.55, 0.845), fontsize=12)\n",
        "\n",
        "  fig.tight_layout()\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hy_3fSEuKsBh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_histograms(Train_Sample, Validation_Sample, Test_Sample, Features, Target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpIxJmsLKubl",
        "colab_type": "text"
      },
      "source": [
        "#### Pre-processing data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ioi3EtQCKyWO",
        "colab_type": "text"
      },
      "source": [
        "As we can see from the histograms, each feature spans a different range of values:\n",
        "\n",
        "*   Longitude has negative values;\n",
        "*   Population is in the order of tens of thousands\n",
        "*   Median house value can reach hundreds of thousands\n",
        "\n",
        "We need to standardize our dataset before presenting it to the model in order to facilitate the training (and convergence).\n",
        "\n",
        "To do this, we use sklearn's StandardScaler function.\n",
        "***\n",
        "The StandardScaler function subtracts the mean $\\mu$ of each column and divides by its stardard deviation $\\sigma$, so that all columns have a mean of zero and stardard deviation of unity. Mathematically, it is described as:\n",
        "\\begin{equation}\n",
        "X' = \\frac{(X - \\mu)}{\\sigma},\n",
        "\\end{equation}\n",
        "where $X'$ is the new column and $X$ is the old column. This process occurs inplace, so the new column will replace the old one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8b70bO7K0mQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Create our scaling function with default values:\n",
        "Scaler_X = StandardScaler()\n",
        "Scaler_Y = StandardScaler()\n",
        "\n",
        "# Scaling our training data:\n",
        "Scaled_Train_X = Scaler_X.fit_transform(Train_Sample[Features])\n",
        "Scaled_Train_Y = Scaler_Y.fit_transform(Train_Sample[Target])\n",
        "\n",
        "# Scaling our validation data:\n",
        "Scaled_Validation_X = Scaler_X.transform(Validation_Sample[Features])\n",
        "Scaled_Validation_Y = Scaler_Y.transform(Validation_Sample[Target])\n",
        "\n",
        "# Now we use the values obtained by the fit on the training data to scale our testing data:\n",
        "Scaled_Test_X = Scaler_X.transform(Test_Sample[Features])\n",
        "Scaled_Test_Y = Scaler_Y.transform(Test_Sample[Target])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfBTsxbjK2Z0",
        "colab_type": "text"
      },
      "source": [
        "Now we can finally start creating our model!\n",
        "***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IjPIEwPpK6ES",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#### Creating a deep-learning model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YrUWXG2K_CT",
        "colab_type": "text"
      },
      "source": [
        "The process of creating a model with Keras is really simple, it is almost as if we were playing with Legos.\n",
        "\n",
        "<img src=\"Extra/KerasLego.png\" style=\"width: 500px;\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUYwVGUVLBh-",
        "colab_type": "text"
      },
      "source": [
        "First, we need to import TensorFlow v2 (which includes Keras):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-eTFLKGLDeR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkWnTD6TLFgt",
        "colab_type": "text"
      },
      "source": [
        "Then we define the method we want to use to create our network. The options are:\n",
        "1. Sequential API: Allows to create the network layer-by-layer. It's the easiest way to create a neural network, but does not allow the creation of shared layers or networks with multiple inputs and/or outputs.\n",
        "2. Functional API: A little more complex than the Sequential API, but still easy to use. Offers the capability of creating networks with branches and multiple inputs and/or outputs.\n",
        "3. Model Subclassing API: Way more dificult to use, but allows the creation of fully customizable and complex architectures, including custom layers and different ways to treat the data.\n",
        "***\n",
        "For this hands-on, we will use the Sequential method.\n",
        "\n",
        "We start defining our network by selecting the method we will use, using ```model = Sequential()``` in our case. Then we add layers the way we want using the ```model.add(...)``` function and the final step is to compile the model using ```model.compile(...)```.\n",
        "\n",
        "***\n",
        "***Hands-on***: Complete the model below the way you want. \n",
        "1. Add two or three Dense layers with any number of neurons between 16 and 128 with a 'relu' activation.\n",
        "    * Try to use multiples of 2 neurons (16, 32, 64, 128, ...)\n",
        "2. Add a Dropout layer between the Dense layers. Try a dropout rate between 0.1 and 0.3.\n",
        "3. Choose the parameters to compile the model.\n",
        "    * Loss: 'mean_squared_error' or 'mean_absolute_deviation' will work nicely.\n",
        "    * Metrics: In case you want to monitor other information, you can define it here as a list (metrics=['mean_absolute_deviation'], for example).\n",
        "    * Optimizer: How the network will optimize the weights. You can choose from 'adam', 'nadam', 'rmsprop', or 'sgd', among others.\n",
        "***\n",
        "***Tips***\n",
        "* A Dense layer is written as ```Dense(units=n, activation='name')```.\n",
        "* A Dropout layer is written as ```Dropout(rate=n)```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjDGDoxtLJJP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining some names to make the notation friendlier\n",
        "\n",
        "Sequential = tf.keras.Sequential\n",
        "Dense      = tf.keras.layers.Dense\n",
        "Dropout    = tf.keras.layers.Dropout\n",
        "\n",
        "def My_Model():\n",
        "    # Define a Sequential model:\n",
        "    model = Sequential()\n",
        "    \n",
        "    # Input Layer\n",
        "    model.add(Dense(units=8, input_dim=8, activation='linear'))\n",
        "\n",
        "    # Hidden Layers\n",
        "    model.add(...)\n",
        "    \n",
        "    # Output Layer\n",
        "    model.add(Dense(units=1, activation='linear'))\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(loss=..., \n",
        "                  optimizer=...)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Lets see a summary of our model:\n",
        "My_Model().summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yu8aGNZrLSyW",
        "colab_type": "text"
      },
      "source": [
        "#### Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbCaooeyLU4L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compiling the model we just defined:\n",
        "MLP_Model = My_Model()\n",
        "\n",
        "# Fitting the model\n",
        "MLP_Model_Fit = MLP_Model.fit(Scaled_Train_X, Scaled_Train_Y, \n",
        "                              validation_data=[Scaled_Validation_X, Scaled_Validation_Y],\n",
        "                              epochs=100, batch_size=256, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wY3DpHRhLYH4",
        "colab_type": "text"
      },
      "source": [
        "When the model is trained, we can easily save it to use later with\n",
        "```\n",
        "MLP_Model.save('Saved_Model.h5')\n",
        "```\n",
        "\n",
        "To load it later and make prediction normally, we use the 'load_model' function:\n",
        "```\n",
        "Loaded_Model = tf.keras.models.load_model('Saved_Model.h5')\n",
        "```\n",
        "\n",
        "***\n",
        "\n",
        "Now lets verify if the training process went well by analysing the loss plots."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_WTjYXeLa6N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.subplots(figsize=(7,5))\n",
        "\n",
        "X = MLP_Model_Fit.epoch\n",
        "\n",
        "plt.plot(X, MLP_Model_Fit.history['loss'], lw=3, label='Training')\n",
        "plt.plot(X, MLP_Model_Fit.history['val_loss'], lw=3, label='Validation')\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "plt.ylabel('Loss (MSE)')\n",
        "plt.xlabel('Iterations')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8Rweo2_LirI",
        "colab_type": "text"
      },
      "source": [
        "#### Making predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5phksHfLl-d",
        "colab_type": "text"
      },
      "source": [
        "Making predictions are as simple as calling a function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmnbNe4YLncw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Making predictions for the test set:\n",
        "Predictions = MLP_Model.predict(Scaled_Test_X)\n",
        "\n",
        "# Since we used the StandardScaler function to normalize all data, we need to do the inverse transform to get the actual house values:\n",
        "Predictions = Scaler_Y.inverse_transform(Predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wBLsfb9LqY6",
        "colab_type": "text"
      },
      "source": [
        "Finally, we can compare our predictions against the real house values on the test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q78MrlJeLr9X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "(1.48*np.median(abs((Predictions - Test_Sample[Target]) - np.median((Predictions - Test_Sample[Target]))) / (1 + Test_Sample[Target]))*100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LX1P-Z7fLtqJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.subplots(figsize=(7,7))\n",
        "plt.hexbin(Test_Sample[Target], Predictions, bins='log', cmap='Greys', gridsize=50)\n",
        "\n",
        "plt.xlabel('True values')\n",
        "plt.ylabel('Predicted values')\n",
        "\n",
        "plt.plot([0,500000], [0,500000], '--', color='red')\n",
        "plt.axis([0,500000, 0, 500000])\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRL_7q8aL1sl",
        "colab_type": "text"
      },
      "source": [
        "### Classification with Keras (Convolutional Neural Networks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUagJbKEL7D6",
        "colab_type": "text"
      },
      "source": [
        "#### Loading the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ur-q2oTL91r",
        "colab_type": "text"
      },
      "source": [
        "For this part of the lesson, we will use the MNIST dataset. This dataset contains 70000 rows, each one representing the image of a handwritten number.\n",
        "\n",
        "The dataset contains:\n",
        "* One label column, with the number represented in the image;\n",
        "* 784 (28x28) columns with the information of the pixels in the image.\n",
        "\n",
        "Each image has 28x28 pixels, and the value of each pixel ranges from 0 to 255."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qcxyPA3RL_y4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We will load the data just like we did before:\n",
        "import pandas as pd\n",
        "\n",
        "# Importing the training dataset:\n",
        "Train_Data = pd.read_csv('Data/mnist_train.csv')\n",
        "\n",
        "# And the testing dataset:\n",
        "Test_Sample = pd.read_csv('Data/mnist_test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F_agLi1BMD8D",
        "colab_type": "text"
      },
      "source": [
        "##### Seeing the image (example)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otixsLVWMGqh",
        "colab_type": "text"
      },
      "source": [
        "To see the images, we need to do some extra steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7Zl52DsMH8g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Each row of these datasets represents an image. We need to convert the numbers to an image to visualize it:\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# First, we select the first row of our dataset and all elements after the very first one (which is the label of the image), then we reshape it to a 28x28 format:\n",
        "Image = Train_Data.values[0][1:].reshape(28,28)\n",
        "\n",
        "# Now, we just need to use Matplotlib's 'imshow' function\n",
        "plt.imshow(Image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noxaXAXQMQND",
        "colab_type": "text"
      },
      "source": [
        "#### Defining the datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csBnrg1NMTN6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Just like we did in the regression part, we should now define a new validation dataset using sklearn's 'train_test_split' function:\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# We use this function like this:\n",
        "Train_Sample, Validation_Sample = train_test_split(Train_Data, train_size=0.8, random_state=1882)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7u7CDn2GMUpt",
        "colab_type": "text"
      },
      "source": [
        "#### Pre-processing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upNUauYZMX0X",
        "colab_type": "text"
      },
      "source": [
        "Just like we did with the regression model, we should normalize our data.\n",
        "\n",
        "The dataset we are using contains one column named 'label', which is the number represented in the image. We should not normalize it. Because of this we need to remove this column from our dataset.\n",
        "\n",
        "We can do this with Pandas:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Etzf6F0NMZd-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define a new variable containing only the labels:\n",
        "Train_Labels = Train_Sample['label']\n",
        "\n",
        "# Drop (remove) the label from the original dataset:\n",
        "Train_Sample.drop('label', axis='columns', inplace=True)\n",
        "\n",
        "# Repeat for the remaining datasets:\n",
        "Validation_Labels = Validation_Sample['label']\n",
        "Validation_Sample.drop('label', axis='columns', inplace=True)\n",
        "\n",
        "Test_Labels = Test_Sample['label']\n",
        "Test_Sample.drop('label', axis='columns', inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qwdSrSiMazI",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "\n",
        "Now, instead of using the StandardScaler function, we will use the MinMaxScaler function.\n",
        "The MinMaxScaler is a little different. This function brings the values of a column to the range $[0, 1]$ by doing two steps:\n",
        "- 1st step: Calculate the scale: $\\text{Scale} = (\\text{max} - \\text{min}) / (X_\\text{max} - X_\\text{min})$\n",
        "- 2nd step: Apply the transformation: $X' = \\text{Scale} \\times X + \\text{min} - X_\\text{min} \\times \\text{Scale}$\n",
        "***\n",
        "In our case, this is:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWBE6ysVMeFV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Create our scaling function with default values:\n",
        "Scaler = MinMaxScaler()\n",
        "\n",
        "# Scaling our training data:\n",
        "Scaled_Train_X = Scaler.fit_transform(Train_Sample)\n",
        "\n",
        "# Scaling our validation data:\n",
        "Scaled_Validation_X = Scaler.transform(Validation_Sample)\n",
        "\n",
        "# Now we use the values obtained by the fit on the training data to scale our testing data:\n",
        "Scaled_Test_X = Scaler.transform(Test_Sample)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6rsBnIwMgg7",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "\n",
        "A Convolutional Neural Network works best with images with three dimensions (height, width and depth), but our data is composed of 1D vectors. We need to reshape the data before handling it to our model:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4BHmBjstMibF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing Numpy to correct the dimension of the images\n",
        "import numpy as np\n",
        "\n",
        "# For the training sample, we first create and empty list:\n",
        "Training_Images = []\n",
        "\n",
        "# Now we will loop over all rows in our dataset\n",
        "for i in range(len(Scaled_Train_X)):\n",
        "  # The row 'i' will be reshaped to form a 28x28 array:\n",
        "  Reshaped_Data = Scaled_Train_X[i].reshape(28,28)\n",
        "  # And we append the reshaped row to the list we just created:\n",
        "  Training_Images.append(Reshaped_Data)\n",
        "\n",
        "# We reshaped our 1D data into a 2D image, but CNNs use 3D images as input (height, width and depth), so we use the 'expand_dims' to turn our images into 3D vectors.\n",
        "Training_Images = np.expand_dims(Training_Images, axis=3)\n",
        "  \n",
        "# Repeat for the validation sample:\n",
        "Validation_Images = []\n",
        "\n",
        "for i in range(len(Scaled_Validation_X)):\n",
        "  Reshaped_Data = Scaled_Validation_X[i].reshape(28,28)\n",
        "  Validation_Images.append(Reshaped_Data)\n",
        "  \n",
        "Validation_Images = np.expand_dims(Validation_Images, axis=3)\n",
        "  \n",
        "# And for the testing sample:\n",
        "Testing_Images = []\n",
        "\n",
        "for i in range(len(Scaled_Test_X)):\n",
        "  Reshaped_Data = Scaled_Test_X[i].reshape(28,28)\n",
        "  Testing_Images.append(Reshaped_Data)\n",
        "  \n",
        "Testing_Images = np.expand_dims(Testing_Images, axis=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xOl6ERjMlQ8",
        "colab_type": "text"
      },
      "source": [
        "***\n",
        "\n",
        "The task we have at hand is considered a 'multi-class' classification because we want to classify images in numbers between 0 and 9. For this reason we need to 'one hot encode' our label vectors.\n",
        "\n",
        "Why we need to do this 'one hot encoding'? Simple, when the neural networks sees categorical labels (from 0 to 9) it 'thinks' that 9 is the most important class, followed by 8 and so on. Due to this we transform these categorical labels into a one hot vector, like this:\n",
        "\n",
        "***\n",
        "$\n",
        "\\begin{bmatrix}\n",
        "\\textbf{Categorical label}\\\\ \\hline\n",
        "1\\\\ \n",
        "8\\\\ \n",
        "5\\\\ \n",
        "3\\\\ \n",
        "2\n",
        "\\end{bmatrix}\n",
        "\\rightarrow\n",
        "\\begin{bmatrix}\n",
        "\\mathbf{0} &\\mathbf{1}  &\\mathbf{2}  &\\mathbf{3}  &\\mathbf{4}  &\\mathbf{5}  &\\mathbf{6}  &\\mathbf{7}  &\\mathbf{8}  &\\mathbf{9} \\\\ \\hline\n",
        "0 &1  &0  &0  &0  &0  &0  &0  &0  &0 \\\\ \n",
        "0 &0  &0  &0  &0  &0  &0  &0  &1  &0 \\\\ \n",
        "0 &0  &0  &0  &0  &1  &0  &0  &0  &0 \\\\ \n",
        "0 &0  &0  &1  &0  &0  &0  &0  &0  &0 \\\\ \n",
        "0 &0  &1  &0  &0  &0  &0  &0  &0  &0 \n",
        "\\end{bmatrix}\n",
        "$\n",
        "***\n",
        "\n",
        "Now the network will not give more importance to one label over another!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-sKJ3HhNGsY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To do the 'one hot encoding'\n",
        "import tensorflow as tf\n",
        "\n",
        "# For the training sample\n",
        "Train_Labels = tf.keras.utils.to_categorical(Train_Labels)\n",
        "\n",
        "# For the validation sample\n",
        "Validation_Labels = tf.keras.utils.to_categorical(Validation_Labels)\n",
        "\n",
        "# For the testing sample\n",
        "Test_Labels = tf.keras.utils.to_categorical(Test_Labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7yEzENKNJmT",
        "colab_type": "text"
      },
      "source": [
        "We can finally create our model, just like we did before, with a few changes in the network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Fa-eAeVNMIS",
        "colab_type": "text"
      },
      "source": [
        "#### Creating a CNN model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZE2xF6uONOoz",
        "colab_type": "text"
      },
      "source": [
        "The main difference between a multi-layer perceptron and a convolutional neural network is the presence of convolutional layers.\n",
        "\n",
        "The main configuration of Dense layers are the number of neurons and the activation function. For Conv2D layers, we need:\n",
        "* The number of filters;\n",
        "* The kernel size;\n",
        "* The stride;"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31yKilbwNRuM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining some names to make the notation friendlier\n",
        "\n",
        "Sequential   = tf.keras.Sequential\n",
        "Dense        = tf.keras.layers.Dense\n",
        "Dropout      = tf.keras.layers.Dropout\n",
        "Conv2D       = tf.keras.layers.Conv2D\n",
        "Flatten      = tf.keras.layers.Flatten\n",
        "MaxPooling2D = tf.keras.layers.MaxPooling2D\n",
        "\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "def My_Model():\n",
        "    # Define a Sequential model:\n",
        "    model = Sequential()\n",
        "    \n",
        "    # Input Layer\n",
        "    model.add(Conv2D(filters=32, kernel_size=(3,3), padding='same', activation='relu', input_shape=(28, 28, 1)))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(Dropout(0.20))\n",
        "\n",
        "    # Hidden Layers\n",
        "    model.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(Dropout(0.20))\n",
        "    \n",
        "    model.add(Conv2D(filters=64, kernel_size=(3,3), activation='relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "    model.add(Dropout(0.20))\n",
        "    \n",
        "    # The final part of a CNN is a group of fully-connected layers (Dense), so the networks can join all the information learned and give an output. \n",
        "    # The 'Flatten' layer is responsible for reshaping the information from the Conv2D layers (which are 3D) into the format that Dense layers use (which is 1D).\n",
        "    model.add(Flatten())\n",
        "    \n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    model.add(Dropout(0.20))\n",
        "    \n",
        "    # Output Layer\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(loss='categorical_crossentropy', \n",
        "                  metrics=['accuracy'], \n",
        "                  optimizer='nadam')\n",
        "\n",
        "    return model\n",
        "\n",
        "# Lets see a summary of our model:\n",
        "My_Model().summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gul9_qkENUCF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compiling the model we just defined:\n",
        "CNN_Model = My_Model()\n",
        "\n",
        "# Fitting the model\n",
        "CNN_Model_Fit = CNN_Model.fit(Training_Images, Train_Labels,\n",
        "                              validation_data=[Validation_Images, Validation_Labels],\n",
        "                              epochs=3, batch_size=256, verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbVK9FzFNVns",
        "colab_type": "text"
      },
      "source": [
        "#### Making predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQCpf5h2NXN6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Making predictions for the test set:\n",
        "Predictions  = CNN_Model.predict(Testing_Images)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjxmWq-sNZ7s",
        "colab_type": "text"
      },
      "source": [
        "To check if (or how much) our predictions are correct, we can use the confusion matrix:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsQx1ALnNarZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# First we import the function from sklearn:\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Then we change our prediction vector a bit, so it is in the format that the 'confusion_matrix' function accepts:\n",
        "Test_True_Labels = np.argmax(Test_Labels, axis=1)\n",
        "Test_Pred_Labels = np.argmax(Predictions, axis=1)\n",
        "\n",
        "# We calculate the confusion matrix:\n",
        "Conf_Matrix = confusion_matrix(Test_True_Labels, Test_Pred_Labels)\n",
        "\n",
        "# And we finally plot it using Seaborn:\n",
        "import seaborn as sn\n",
        "\n",
        "plt.figure(figsize=(7,7))\n",
        "sn.set(font_scale=1.4)\n",
        "sn.heatmap(Conf_Matrix, annot=True, fmt='', annot_kws={\"size\": 12}, cbar=False)\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcwvrB6rNhUo",
        "colab_type": "text"
      },
      "source": [
        "### Exercises\n",
        "\n",
        "1. Create and train a multi-layer perceptron model to predict photometric redshifts on S-PLUS data.\n",
        "    * Evaluate the performance of your model by calculating the stardard deviation ($\\sigma$), bias ($\\mu$) and fraction of outliers ($\\eta$) of your results.\n",
        "\n",
        "\\begin{equation}\n",
        "    \\delta = (\\text{True values} - \\text{Predicted values})\n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "    \\sigma = \\sqrt{\\delta/N}\n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "    \\mu = \\overline{\\delta}\n",
        "\\end{equation}\n",
        "\\begin{equation}\n",
        "    \\eta = \\text{Fraction of objects for which} \\delta \\geq 0.15\n",
        "\\end{equation}\n",
        "\n",
        "2. Create a Convolutional Neural Network to classify the Fashion MNIST dataset.\n",
        "    * The dataset is built-in with [Keras](https://keras.io/datasets/#fashion-mnist-database-of-fashion-articles). You can also easily find it on the internet.\n",
        "    * Make a confusion matrix of your results. Your model is predicting correctly?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZwQ5pxYSLiM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}